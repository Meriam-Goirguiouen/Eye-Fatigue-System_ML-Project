{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148d9cf6-c1dc-49b1-9383-b19eac3eabbb",
   "metadata": {},
   "source": [
    "# Projet : Système de détection de la fatigue oculaire\n",
    "\n",
    "## Objectif du projet\n",
    "Ce projet a pour but de développer un modèle de Machine Learning capable de détecter la fatigue oculaire à partir d’images, afin de prévenir les risques liés à la conduite.\n",
    "\n",
    "---\n",
    "## Structure du Notebook\n",
    "\n",
    "Ce notebook est organisé en plusieurs parties principales :\n",
    "\n",
    " 1- Collecte des données\n",
    " \n",
    " 2- Prétraitement des données\n",
    " \n",
    " 3- Entraînement du modèle\n",
    " \n",
    " 4- Évaluation du modèle\n",
    " \n",
    " 5- Visualisation et interprétation\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8376b-c775-4436-9bbe-0b0a1bece6c4",
   "metadata": {},
   "source": [
    "## Collecte des données\n",
    "\n",
    "Cette section contient le code permettant de récupérer et d’organiser les données utilisées dans ce projet.\n",
    "\n",
    "Le dataset provient de la plateforme **Kaggle** et a été téléchargé manuellement, puis décompressé localement dans le répertoire de travail du projet.\n",
    "\n",
    "Chaque sous-partie ci-dessous traite une étape spécifique :\n",
    "- Vérification de la structure des données\n",
    "- Renommer les sous-dossiers pour améliorer la lisibilité\n",
    "- Chargement des images depuis les sous-dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6be6208-f8a3-495d-8d24-9f623e7851d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction en cours...\n",
      "Extraction terminée avec succès !\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Chemin du fichier ZIP\n",
    "zip_path = r'C:\\Users\\hp\\Desktop\\jupyter_notebook\\driver-drowsiness-dataset-ddd.zip'\n",
    "\n",
    "# Répertoire de destination\n",
    "extract_path = r'C:\\Users\\hp\\Desktop\\jupyter_notebook'\n",
    "\n",
    "# Vérifier si le fichier existe avant de l'extraire\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        print(\"Extraction en cours...\")\n",
    "        zip_ref.extractall(extract_path)\n",
    "        print(\"Extraction terminée avec succès !\")\n",
    "else:\n",
    "    print(f\"Le fichier {zip_path} n'existe pas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dbb0d8-f7b4-4f85-a8f7-7a9a2afb1bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'Drowsy' n'existe pas.\n",
      "Le dossier 'Non Drowsy' n'existe pas.\n",
      "\n",
      "Contenu du dossier après renommage :\n",
      "['yeux_fermés', 'yeux_ouverts']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Chemin du dataset\n",
    "dataset_path = r'C:\\Users\\hp\\Desktop\\jupyter_notebook\\Driver Drowsiness Dataset (DDD)'\n",
    "\n",
    "# --- Renommer le dossier 'Drowsy' en 'yeux_fermés'\n",
    "try:\n",
    "    os.rename(os.path.join(dataset_path, 'Drowsy'), os.path.join(dataset_path, 'yeux_fermés'))\n",
    "    print(\"Dossier 'Drowsy' renommé en 'yeux_fermés'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Le dossier 'Drowsy' n'existe pas.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du renommage de 'Drowsy' : {e}\")\n",
    "\n",
    "# --- Renommer le dossier 'Non Drowsy' en 'yeux_ouverts'\n",
    "try:\n",
    "    os.rename(os.path.join(dataset_path, 'Non Drowsy'), os.path.join(dataset_path, 'yeux_ouverts'))\n",
    "    print(\"Dossier 'Non Drowsy' renommé en 'yeux_ouverts'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Le dossier 'Non Drowsy' n'existe pas.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du renommage de 'Non Drowsy' : {e}\")\n",
    "\n",
    "# Vérifier le résultat\n",
    "print(\"\\nContenu du dossier après renommage :\")\n",
    "print(os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d484c0-8f02-49fd-bcbd-2182a81d9a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des images de la classe : yeux_fermés\n"
     ]
    }
   ],
   "source": [
    "# --- Chargement des images depuis les sous-dossiers :\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = r'C:\\Users\\hp\\Desktop\\jupyter_notebook\\Driver Drowsiness Dataset (DDD)'\n",
    "\n",
    "# Liste pour stocker les images\n",
    "images = []\n",
    "# Liste pour stocker les étiquettes correspondantes\n",
    "labels = []\n",
    "\n",
    "# NB: Chaque sous-dossier correspond à une classe :\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if os.path.isdir(folder_path):  # Vérifier si c'est un dossier\n",
    "        print(f\"Chargement des images de la classe : {folder}\")\n",
    "        \n",
    "        # Parcourir chaque image dans le sous-dossier\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    # Ajouter l'image et son label (nom du sous-dossier) aux listes\n",
    "                    images.append(img)\n",
    "                    labels.append(folder)  # L'étiquette est le nom du sous-dossier\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur avec l'image {img_path}: {e}\")\n",
    "\n",
    "# Afficher le nombre d'images collectées\n",
    "print(f\"Nombre total d'images collectées : {len(images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a667d-b2df-4cee-a97a-02e9af2b6930",
   "metadata": {},
   "source": [
    "## Prétraitement des données\n",
    "\n",
    "Dans cette section, nous préparons les données brutes pour notre modèle de Machine Learning afin de garantir que ce dernier reçoive des données propres, cohérentes et exploitables.\n",
    "\n",
    "Les étapes de prétraitement appliquées sont les suivantes :\n",
    "\n",
    "### 1- Redimensionnement des images\n",
    "Toutes les images seront redimensionnées à une taille fixe (par exemple 64x64 pixels) afin d’uniformiser les dimensions et réduire le coût de calcul.\n",
    "\n",
    "### 2- Normalisation\n",
    "Les valeurs des pixels seront normalisées (généralement entre 0 et 1) pour faciliter l’apprentissage du modèle et améliorer la convergence.\n",
    "\n",
    "### 3- Attribution des étiquettes\n",
    "Chaque image sera associée à une étiquette numérique selon sa classe :\n",
    "- `0` : yeux fermés\n",
    "- `1` : yeux ouverts\n",
    "\n",
    "### 4- Division en jeux d’entraînement et de test\n",
    "Les données seront divisées en deux ensembles :\n",
    "- **Entraînement** : pour que le modèle apprenne à reconnaître les classes\n",
    "- **Test** : pour évaluer la performance du modèle sur des données jamais vues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ceb1b21-e2c0-4120-9bac-bd949b0a7566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitement des images de la classe : yeux_fermés\n",
      "Prétraitement des images de la classe : yeux_ouverts\n",
      "Prétraitement terminé !\n",
      "Nombre d'images dans le jeu d'entraînement : 33434\n",
      "Nombre d'images dans le jeu de test : 8359\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chemin du dataset\n",
    "dataset_path = r'C:\\Users\\hp\\Desktop\\jupyter_notebook\\Driver Drowsiness Dataset (DDD)'\n",
    "\n",
    "# Dimensions des images\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "# Dictionnaire pour mapper les labels\n",
    "label_map = {\n",
    "    'yeux_fermés': 0,\n",
    "    'yeux_ouverts': 1\n",
    "}\n",
    "\n",
    "# Listes pour stocker les données et les labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Parcourir les dossiers et charger les images\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f'Prétraitement des images de la classe : {folder}')\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            try:\n",
    "                # Ouvrir l'image et la redimensionner\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.resize(IMG_SIZE)\n",
    "                    img_array = np.array(img) / 255.0  # Normalisation\n",
    "\n",
    "                    # Vérifier que l'image est bien en format RGB\n",
    "                    if img_array.shape == (64, 64, 3):\n",
    "                        data.append(img_array)\n",
    "                        labels.append(label_map[folder])\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur avec l'image {img_path}: {e}\")\n",
    "\n",
    "# Conversion en tableaux NumPy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Prétraitement terminé !\")\n",
    "print(f\"Nombre d'images dans le jeu d'entraînement : {len(X_train)}\")\n",
    "print(f\"Nombre d'images dans le jeu de test : {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da07f623-01b2-4464-aff3-880593aaecb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# ---- Entraînement du modèle ----\n",
    "\n",
    "# Définir le modèle CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save('model_detection_fatigue.h5')\n",
    "\n",
    "print(\"Entraînement terminé et modèle sauvegardé !\")\n",
    "\n",
    "# ---- Évaluation du modèle ----\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = load_model('model_detection_fatigue.h5')\n",
    "\n",
    "# Évaluer la performance du modèle\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Prédictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=['yeux_fermés', 'yeux_ouverts'], yticklabels=['yeux_fermés', 'yeux_ouverts'])\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Vraies classes')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()\n",
    "\n",
    "print(\"Évaluation du modèle terminée !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7eb7ad-4130-4503-80a1-a2f36bf2a7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
